---
author:
  - name: Viviane Philipps
    affiliation: |
      | Inserm, Bordeaux Population Health Research Center, UMR 1219, 
      | Univ. Bordeaux, F-33000 Bordeaux, France \AND
    address: |
      | 146 rue Léo Saignat
      | 33076 Bordeaux Cedex
      | France
    email: \email{viviane.philipps@u-bordeaux.fr}
  - name: Boris P. Hejblum 
    affiliation: ''
  - name: Mélanie Prague
    affiliation: |
      | Univ. Bordeaux, Inserm, Bordeaux Population Health Research Center, 
      | UMR 1219, Inria BSO SISTM, F-33000 Bordeaux, France
  - name: Daniel Commenges  
    affiliation: '\AND'
  - name: Cécile Proust-Lima
    affiliation: |
      | Inserm, Bordeaux Population Health Research Center, 
      | UMR 1219, Univ. Bordeaux, F-33000 Bordeaux, France
#  - name:
#    affiliation: ''
title:
  formatted: "Robust and Efficient Optimization Using a Marquardt-Levenberg Algorithm with \\proglang{R} Package \\pkg{marqLevAlg}"
  plain:     "Robust and Efficient Optimization Using a Marquardt-Levenberg Algorithm with R Package marqLevAlg"
  short:     "The \\proglang{R} package \\pkg{marqLevAlg}"
abstract: >
  Optimization is an essential task in many computational problems. In statistical modelling for instance, in the absence of analytical solution, maximum likelihood estimators are often retrieved using iterative optimization algorithms. R software already includes a variety of optimizers from general-purpose optimization algorithms to more specific ones. Among Newton-like methods which have good convergence properties, the Marquardt-Levenberg algorithm (MLA) provides a particularly robust algorithm for solving optimization problems.  Newton-like methods generally have two major limitations: (i) convergence criteria that are a little too loose, and do not ensure convergence towards a maximum, (ii) a calculation time that is often too long, which makes them unusable in complex problems. We propose in the marqLevAlg package an efficient and general implementation of a modified MLA combined with strict convergence criteria and parallel computations. Convergence to saddle points is avoided by using the relative distance to minimum/maximum criterion (RDM) in addition to the stability of the parameters and of the objective function. RDM exploits the first and second derivatives to compute the distance to a true local maximum. The independent multiple evaluations of the objective function at each iteration used for computing either first or second derivatives are called in parallel to allow a theoretical speed up to the square of the number of parameters. We show through the estimation of 7 relatively complex statistical models how parallel implementation can largely reduce computational time. We also show through the estimation of the same model using 3 different algorithms (BFGS of optim routine, an E-M, and MLA) the superior efficiency of MLA to correctly and consistently reach the maximum. 
keywords:
  formatted: [convergence criteria, Marquardt-Levenberg, Newton-Raphson, optimization, parallel computing, R]
  plain:     [convergence criteria, Marquardt-Levenberg, Newton-Raphson, optimization, parallel computing, R]
preamble: >
  \usepackage{amsmath}
  \usepackage{amssymb}
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
documentclass: jss
classoption: nojss
output: rticles::jss_article
vignette: >
  %\VignetteIndexEntry{MLA}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: mla.bib
---
\newpage

# Introduction

Optimization is an essential task in many computational problems. In statistical modelling for instance, in the absence of analytical solution, maximum likelihood estimators are often retrieved using iterative optimization algorithms.

Steepest descent algorithms are among the most famous general optimization algorithms. They generally consist in updating parameters according to the steepest gradient (gradient descent) possibly scaled by the Hessian in the Newton (Newton-Raphson) algorithm or an approximation of the Hessian based on the gradients in the quasi-Newton algorithms (e.g., Broyden-Fletcher-Goldfarb-Shanno \--- BFGS). Newton-like algorithms have been shown to provide good convergence properties [@joe_numerical_2003] and were demonstrated in particular to behave better than Expectation-Maximization (EM) algorithms in several contexts of Maximum Likelihood Estimation, such as the random-effect models [@lindstrom_newtonraphson_1988] or the latent class models [@proust_estimation_2005]. Among Newton methods, the Marquardt-Levenberg algorithm, initially proposed by Levenberg [@levenberg_method_1944] then Marquardt [@marquardt_algorithm_1963], combines BFGS and gradient descent methods to provide a more robust optimization algorithm. 

The \proglang{R} software includes multiple solutions for optimization tasks (see CRAN task View on ``Optimization and Mathematical Programming'' [@optimization]). In particular the \code{optim} function in \pkg{base} \proglang{R} offers different algorithms for general purpose optimization, and so does \code{optimx} \--- a more recent package extending \code{optim}  [@nash_2011]. Numerous additional packages are available for different contexts, from nonlinear least square problems (including some exploiting Marquardt-Levenberg idea \--- \pkg{minpack.lm} [@elzhov_2016]) to stochastic optimization and algorithms based on the simplex approach. However, \proglang{R} software could benefit from a general-purpose \proglang{R} implementation of Marquardt-Levenberg algorithm. 

Moreover, while optimization can be easily achieved in small dimension, the increasing complexity of statistical models leads to critical issues. First, the large dimension of the objective function can induce excessively long computation times. Second, with complex objective functions, it is more likely to encounter flat regions, so that convergence cannot be assessed according to objective function stability anymore. 

To address these two issues, we propose a \proglang{R} implementation of the Levenberg-Marquardt algorithm in the package \pkg{marqLevAlg} which relies on a stringent convergence criterion based on the first and second derivatives to avoid loosely convergence [@prague:hal-00717566] and includes (from version 2.0.1) parallel computations within each iteration to speed up convergence in complex settings. 

Section 2 and 3 describe the algorithm and the implementation, respectively. Then Section 4 provides an example of call with the estimation of a linear mixed model. A benchmark of the package is reported in Section 5 with the performances of parallel implementation. Marquardt-Levenberg algorithm implementation is also compared with other algorithms on a case example in Section 6. Finally Section 7 concludes.


# Methodology

## The Marquardt-Levenberg algorithm

The Marquardt-Levenberg algorithm (MLA) can be used for any problem where a function $\mathcal{Q}(\theta)$ has to be minimized (or equivalently, function $\mathcal{L}(\theta)$= - $\mathcal{Q}(\theta)$ has to be maximized) according to a set of $m$ unconstrained parameters $\theta$, as long as the second derivatives of $\mathcal{Q}(\theta)$ exist. In statistical applications for instance, the objective function is the deviance to be minimized or the log-likelihood to be maximized. 

Our improved MLA iteratively updates the vector $\theta^{(k)}$ from a starting point $\theta^{(0)}$ until convergence using the following formula at iteration $k+1$: 


$$\theta^{(k+1)}=\theta^{(k)}-\delta_{k} (\tilde{H}^{(k)})^{-1}\nabla(\mathcal{Q}(\theta^{(k)}))$$

where $\theta^{(k)}$ is the set of parameters at iteration $k$, $\nabla(\mathcal{Q}(\theta^{(k)}))$ is the gradient of the objective function at iteration $k$, and $\tilde{H}^{(k)}$ is the Hessian matrix $H^{(k)}$  where the diagonal terms are replaced by $\tilde{H}_{ii}^{(k)}=H_{ii}^{(k)}+\lambda_k[(1-\eta_k)|H_{ii}^{(k)}|+\eta_k \text{tr}(H^{(k)})]$. In the original MLA the Hessian matrix is inflated by a scaled identity matrix. Following @fletcher_modified_1971 we consider a refined inflation based on the curvature. The diagonal inflation of our improved MLA makes it an intermediate between the steepest descent method and the Newton method. The parameters $\delta_k$, $\lambda_k$ and $\eta_k$ are scalars specifically determined at each iteration $k$. Parameter $\delta_k$ is fixed to 1 unless the objective function is not reduced, in which case a line search determines the locally optimal step length. Parameters $\lambda_k$ and $\eta_k$ are internally modified in order to ensure that (i) $\tilde{H}^{(k)}$ be definite-positive at each iteration $k$, and (ii) $\tilde{H}^{(k)}$ approaches ${H}^{(k)}$ when $\theta_k$ approaches $\hat{\theta}$.

When the problem encounters a unique solution, the minimum is reached whatever the chosen initial values.


## Stringent convergence criteria {#sec:criteria}

As in any iterative algorithm, convergence of MLA is achieved when convergence criteria are fullfilled. In \pkg{marqLevAlg} package, convergence is defined according to three criteria:

- parameters stability: $\sum_{j=1}^{m} (\theta_{j}^{(k+1)}-\theta_{j}^{(k)})^2 < \epsilon_a$

- objective function stability: $|\mathcal{Q}^{(k+1)} - \mathcal{Q}^{(k)}| < \epsilon_b$

- relative distance to minimum/maximum (RDM):
$\frac{\nabla(\mathcal{Q}(\theta^{(k)})) (H^{(k)})^{-1} \nabla(\mathcal{Q}(\theta^{(k)})) }{m} < \epsilon_d$

The last criterion is essential to ensure that an optimum is truly reached. Indeed, the two first criteria used in other iterative algorithms only ensure that the algorithm reached a saddle point. As the last criterion based on the derivatives requires the Hessian matrix to be invertible, it prevents from such convergences to a saddle point. When the Hessian is not invertible, RDM is set to 1+$\epsilon_d$ and convergence criteria cannot be fullfilled. 

Although it constitutes a relevant convergence criterion in any optimization context, RDM was initially designed for log-likelihood maximization problems, that is cases where $\mathcal{Q}(\theta)$= - $\mathcal{L}(\theta)$ with $\mathcal{L}$ the log-likelihood. In that context, RDM can be interpreted as the ratio between the numerical error and the statistical error [@commenges_rvs_2006,@prague2013nimrod].

The three thresholds $\epsilon_a$, $\epsilon_b$ and $\epsilon_d$ can be adjusted, but values around $0.001$ are usually sufficient to guarantee a correct convergence. In some complex loglikelihood maximisation problems for instance, @prague2013nimrod showed that the RDM convergence properties remain acceptable providing $\epsilon_d$ is below 0.1 (although the lower the better).

## Derivatives calculation

MLA update relies on first ($\nabla(\mathcal{Q}(\theta^{(k)}))$) and second (${H}^{(k)}$) derivatives of the objective function $\mathcal{Q}(\theta^{(k)})$ at each iteration k. The gradient and the Hessian may sometimes be calculated analytically but in a general framework, numerical approximation can become necessary. In \pkg{marqLevAlg} package, in the absence of analytical gradient computation, the first derivatives are computed by central finite differences. In the absence of analytical Hessian, the second derivatives are computed using forward finite differences. The step of finite difference for each derivative depends on the value of the involved parameter. It is set to $\max(10^{-7},10^{-4}|\theta_j|)$ for parameter $j$.

When both the gradient and the Hessian are to be numerically computed, numerous evaluations of $\mathcal{Q}$ are required at each iteration: 

- $2\times m$ evaluations of $\mathcal{Q}$ for the numerical approximation of the gradient function;

- $\dfrac{m \times (m+1)}{2}$ evaluations of $\mathcal{Q}$ for the numerical approximation of the Hessian matrix.

The number of derivatives thus grows quadratically with the number $m$ of parameters and calculations are per se independent as done for different vectors of parameters $\theta$. 

When the gradient is analytically calculated, only the second derivatives have to be approximated, requiring $2 \times m$ independent calls to the gradient function. In that case, the complexity thus linearly increases with $m$. 

In both cases, and especially when each calculation of derivative is long and/or $m$ is large, parallel computations of independent $\mathcal{Q}$ evaluations becomes particularly relevant to speed up the estimation process. 


## Special case of a log-likelihood maximization {#sec:loglik}

When the optimization problem is the maximization of the log-likelihood $\mathcal{L}(\theta)$ of a statistical model according to parameters $\theta$, the Hessian matrix of the $\mathcal{Q}(\theta) = - \mathcal{L}(\theta)$ calculated at the optimum $\hat{\theta}$, $\mathcal{H}_{\hat{\theta}} = - \dfrac{\partial^2 \mathcal{L}(\theta)}{\partial \theta^2} |_{\theta = \hat{\theta}}$, provides an estimator of the Fisher Information matrix. The inverse of $\mathcal{H}_{\hat{\theta}}$ computed in the package thus provides an estimator of the variance-covariance matrix of the optimized vector of parameters $\hat{\theta}$.



# Implementation

## marqLevAlg function

The call of the \code{marqLevAlg} function, or its shorcut \code{mla}, is the following :

\begin{verbatim}
marqLevAlg(b, m = FALSE, fn, gr = NULL, hess = NULL, maxiter = 500,
  epsa = 0.001, epsb = 0.001, epsd = 0.01, digits = 8,
  print.info = FALSE, blinding = TRUE, multipleTry = 25, nproc = 1,
  clustertype = NULL, file = "", .packages = NULL, minimize = TRUE, ...)
\end{verbatim}


Argument \code{b} is the set of initial parameters; alternatively its length \code{m} can be entered. \code{fn} is the function to optimize; it should take the parameter vector as first argument, and additional arguments are passed in \dots . Optional \code{gr} and \code{hess} refer to the functions implementing the analytical calculations of the gradient and the Hessian matrix, respectively. \code{maxiter} is the maximum number of iterations. Arguments \code{epsa}, \code{epsb} and \code{epsd} are the thresholds for the three convergence criteria defined in Section \ref{sec:criteria}. \code{print.info} specifies if details on each iteration should be printed; such information can be reported in a file if argument \code{file} is specified, and \code{digits} indicates the number of decimals in the eventually reported information during optimization. \code{blinding} is an option allowing the algorithm to go on even when the \code{fn} function returns NA, which is then replaced by the arbitrary value of $500,000$ (for minimization) and -$500,000$ (for maximization). Similarly, if an infinite value is found for the chosen initial values, the \code{multipleTry} option will internally reshape \code{b} (up to \code{multipleTry} times) until a finite value is get, and the algorithm can be correctly initialized. The parallel framework is first stated by the \code{nproc} argument which gives the number of cores and by the \code{clustertype} argument (see the next section). In the case where the \code{fn} function depends on \proglang{R} packages, these should be given as a character vector in the \code{.packages} argument. Finally, the \code{minimize} argument offers the possibility to minimize or maximize the objective function \code{fn}; a maximization problem is implemented as the minimization of the opposite function (\code{-fn}).

## Implementation of parallel computations

In the absence of analytical gradient calculation, derivatives are computed in \code{deriva} subfunction with two loops, one for the first derivatives and one for the second derivatives. Both loops are parallelized. The parallelized loops are at most over $m*(m+1)/2$ elements for $m$ parameters to estimate which suggests that the performance could theoretically be improved with up to $m*(m+1)/2$ cores.

When the gradient is calculated analytically, \code{deriva} subfunction is replaced by \code{deriva_grad} subfunction. It is parallelized in the same way but the parallelization being executed over $m$ elements, the performance should be bounded at $m$ cores.

In all cases, the parallelization is achieved using the \pkg{doParallel} and \pkg{foreach} packages. The snow and multicore options of the \code{doParallel} backend are kept, making the parallel option of \pkg{marqLevAlg} package available on all systems. The user specifies the type of parallel environment among FORK, SOCK or MPI in argument \code{clustertype} and the number of cores in \code{nproc}. For instance, \code{clustertype = "FORK", nproc = 6} will use FORK technology and 6 cores. 

# Example 

```{r, results='hide', echo=FALSE, include = FALSE}
knitr::opts_chunk$set(cache = TRUE, comment = '', echo = TRUE)
load("data_mla.RData")
library("marqLevAlg")
library("ggplot2")
library("viridis")
library("patchwork")
```

We illustrate how to use \code{marqLevAlg} function with the maximum likelihood estimation in a linear mixed model [@laird_random-effects_1982]. Function \code{loglikLMM} available in the package implements the log-likelihood of a linear mixed model for a dependent outcome vector ordered by subject (argument $Y$) explained according to a matrix of covariates (argument $X$) entered in the same order as $Y$ with a Gaussian individual-specific random intercept and Gaussian independent errors: 
\begin{verbatim} 
loglikLMM(b, Y, X, ni) 
\end{verbatim}

Argument $b$ specifies the vector of parameters with first the regression parameters (length given by the number of columns in $X$) and then the standard deviations of the random intercept and of the independent error. Finally argument $ni$ specifies the number of repeated measures for each subject. 

We consider the dataset \code{dataEx} (available in the package) in which variable $Y$ is repeatedly observed at time $t$ for 500 subjects along with a binary variable $X1$ and a continuous variable $X3$. For the illustration, we specify a linear trajectory over time adjusted for $X1$, $X3$ and the interaction between $X1$ and time $t$. The vector of parameters to estimate corresponds to the intercept, 4 regression parameters and the 2 standard deviations. 

We first define the quantities to include as argument in \code{loglikLMM} function: 

```{r}
Y <- dataEx$Y
X <- as.matrix(cbind(1, dataEx[, c("t", "X1", "X3")], 
                     dataEx$t * dataEx$X1))
ni <- as.numeric(table(dataEx$i))
```
The vector of initial parameters to specify in \code{marqLevAlg} call is created with the trivial values of 0 for the fixed effects and 1 for the variance components.
```{r}
binit <- c(0, 0, 0, 0, 0, 1, 1)
```

The maximum likelihood estimation of the linear mixed model in sequential mode is then run using a simple call to \code{marqLevAlg} function for a maximization (with argument \code{minimize = FALSE}):
```{r}
estim <- marqLevAlg(b = binit, fn = loglikLMM, minimize = FALSE, 
                    X = X, Y = Y, ni = ni)
estim
```

The printed output \code{estim} shows that the algorithm converged in 18 iterations with convergence criteria of 3.2e-07, 4.35e-06 and 0 for parameters stability, objective function stability and RDM, respectively. The output also displays the list of coefficient values at the optimum. All this information can also be recovered in the \code{estim} object, where item \code{b} contains the estimated coefficients. 

As mentioned in Section \ref{sec:loglik}, in log-likelihood maximization problems, the inverse of the Hessian given by the program provides an estimate of the variance-covariance matrix of the coefficients at the optimum. The upper triangular matrix of the inverse Hessian is thus systematically computed in object \code{v}. When appropriate, the \code{summary} function can output this information with option \code{loglik = TRUE}. With this option, the summary also includes the square root of these variances (i.e., the standards errors), the corresponding Wald statistic, the associated $p$ value and the 95\% confidence interval boundaries for each parameter:

```{r}
summary(estim, loglik = TRUE)
```



The exact same model can also be estimated in parallel mode using FORK implementation of parallelism (here with two cores): 

```{r, warning=FALSE}
estim2 <- marqLevAlg(b = binit, fn = loglikLMM, minimize = FALSE, 
                     nproc = 2, clustertype = "FORK", 
                     X = X, Y = Y, ni = ni)
```

It can also be estimated by using analytical gradients (provided in gradient function \code{gradLMM} with the same arguments as \code{loglikLMM}):

```{r}
estim3 <- marqLevAlg(b = binit, fn = loglikLMM, gr = gradLMM, 
                     minimize = FALSE, X = X, Y = Y, ni = ni)
```

In all three situations, the program converges to the same maximum as shown in Table \ref{tab:fit} for the estimation process and in Table \ref{tab:estim} for the parameter estimates. The iteration process is identical when using the either the sequential or the parallel code (number of iterations, final convergence criteria, etc). It necessarily differs slightly when using the analytical gradient, as the computations steps are not identical (e.g., here it converges in 15 iterations rather than 18) but all the final results are identical.

```{r,echo=FALSE,results='asis'}
res <- function(x, core, gradient){
  res <- data.frame(core = core, gradient = gradient, loglik = x$fn, 
                    iterations = x$ni,
                    criterion1 = x$ca,
                    criterion2 = x$cb,
                    criterion3 = x$rdm)
  rownames(res) <- paste("Object ", deparse(substitute(x)), sep = "")
  colnames(res) <- c("Number of cores", "Analytical gradient", "Objective Function", "Number of iterations", "Parameter Stability", "Likelihood stability", "RDM")
  return(t(res))
}
library("xtable")
print(xtable(cbind(res(estim, 1, "no"), res(estim2, 2, "no"), res(estim3, 1, "yes")), digits = matrix(c(rep(0, 7), 0, 0, 2, 0, -1, -1, -1, 0, 0, 2, 0, -1, -1, -1, 0, 0, 2, 0, -1, -1, -1), 7, 4), label = "tab:fit",
             caption = "Summary of the estimation process of a linear mixed model using marqLevAlg function run either in sequential mode with numerical gradient calculation (object estim), parallel mode with numerical gradient calculation (object estim2), or sequential mode with analytical gradient calculation (object estim3).", align = c("l", "r", "r", "r")), comment = FALSE)
```



  

```{r,echo=FALSE,results='asis'}
coef <- function(x){
  coef <- cbind(x$b, sqrt(x$v[c(1, 3, 6, 10, 15, 21, 28)]))
  colnames(coef) <- c(paste("Coef (", deparse(substitute(x)), ")", sep = ""),
                  paste("SE (", deparse(substitute(x)), ")", sep = ""))
  rownames(coef) <- paste("Parameter", 1:7)  
  return(round(coef, digits = 4))
}

addtorow <- list()
addtorow$pos <- list(-1, 0)
addtorow$command <- c("\\hline & \\multicolumn{2}{r}{Object estim} & \\multicolumn{2}{r}{Object estim2} & \\multicolumn{2}{r}{Object estim3} \\\\", " & Coef & SE &  Coef & SE &  Coef & SE \\\\")

print(xtable(cbind(coef(estim), coef(estim2), coef(estim3)),digits = 4,  label = "tab:estim",
             caption = "Estimates (Coef) and standard error (SE) of the parameters of a linear mixed model fitted using marqLevAlg function run either in sequential mode with numerical gradient calculation (object estim), parallel mode with numerical gradient calculation (object estim2), or sequential mode with analytical gradient calculation (object estim3)."), comment = FALSE, add.to.row = addtorow, include.colnames = FALSE)
```


# Benchmark

We aimed at evaluating and comparing the performances of the parallelization in some time consuming examples. We focused on three examples of sophisticated models from the mixed models area estimated by maximum likelihood. These examples rely on packages using three different languages, thus illustrating the behavior of \pkg{marqLevAlg} package with a program exclusively written in \proglang{R} (\pkg{JM}, @rizopoulos_jm_2010), and programs including  \proglang{Rcpp} (\pkg{CInLPN}, @tadde_dynmod) and \proglang{Fortran90} (\pkg{lcmm}, @proust-lima_lcmm_2017) languages widely used in complex situations.

We first describe the generated dataset on which the benchmark has been realized. We then intoduce each statistical model and associated program. Finally, we detail the results obtained with the three programs. Each time, the model has been estimated sequentially and with a varying number of cores in order to provide the program speed-up. We used a Linux cluster with 32 cores machines and 100 replicates to assess the variability. Codes and dataset used in this section are available at \url{https://github.com/VivianePhilipps/marqLevAlgPaper.}


## Simulated dataset

We generated a dataset of $20,000$ subjects having repeated measurements of a marker \code{Ycens} (measured at times \code{t}) up to a right-censored time of event \code{tsurv} with indicator that the event occured \code{event}. The data were generated according to a 4 latent class joint model [@proust-lima_joint_2014]. This model assumes that the population is divided in 4 latent classes, each class having a specific trajectory of the marker defined according to a linear mixed model with specific parameters, and a specific risk of event defined according to a parametric proportional hazard model with specific parameters too. The class-specific linear mixed model included a basis of natural cubic splines with 3 equidistant knots taken at times 5, 10 and 15, associated with fixed and correlated random-effects. The proportional hazard model included a class-specific Weibull risk adjusted on 3 covariates: one binary (Bernoulli with 50\% probability) and two continous variables (standard Gaussian, and Gaussian with mean 45 and standard deviation 8). The proportion of individuals in each class is about 22\%, 17\%, 34\% and 27\% in the sample.

Below are given the five first rows of the three first subjects:


```{r, echo=FALSE}
data_mla$class <- data_mla$classe
data_mla[c(1:5, 22:26, 43:47), c("i", "class", "X1", "X2", "X3", "t", "Ycens", "tsurv", "event")]
```


## Statistical models

### Joint shared random effect model for a longitudinal marker and a time to event: package JM 

The maximum likelihood estimation of joint shared random effect models has been made available in \proglang{R} with the \pkg{JM} package [@rizopoulos_jm_2010]. The implemented optimization functions are \code{optim} and \code{nlminb}. We added the \code{marqLevALg} function for the purpose of this example. 
We considered a subsample of the simulated dataset, consisting in $5,000$ randomly selected subjects.

The joint shared random effect model is divided into two submodels jointly estimated: 

- a linear mixed submodel for the repeated marker $Y$ measured at different times $t_{ij}$ ($j=1,...,n_i$):
$$\begin{split}
Y_{i}(t_{ij})  &= \tilde{Y}_{i}(t_{ij}) + \varepsilon_{ij}\\
&= X_i(t_{ij}) \beta + Z_i(t_{ij}) u_i + \varepsilon_{ij}
\end{split}$$

where, in our example, $X_i(t)$ contained the intercept, the class indicator, the 3 simulated covariates, a basis of natural cubic splines on time $t$ (with 2 internal knots at times 5 and 15) and the interactions between the splines and the time-invariant covariates, resulting in 20 fixed effects. $Z_i(t)$ contained the intercept and the same basis of natural cubic splines on time $t$, and was associated with $u_i$, the 4-vector of correlated Gaussian random effects.  $\varepsilon_{ij}$ was the independent Gaussian error.


- a survival submodel for the right censored time-to-event: 
\[
 \alpha_i(t) = \alpha_0(t) \exp(X_{si}\gamma + \eta \tilde{Y}_{i}(t))
\]

where, in our example, the vector $X_{si}$, containing the 3 simulated covariates, was associated with the vector of parameters $\gamma$; the current underlying level of the marker $\tilde{Y}_{i}(t))$ was associated with parameter $\eta$ and the baseline hazard $\alpha_{0}(t)$ was defined using a basis of B-splines with 1 interior knot.


The length of the total vector of parameters $\theta$ to estimate was 40 (20 fixed effects and 11 variance component parameters in the longitudinal submodel, and 9 parameters in the survival submodel). 


One particularity of this model is that the log-likelihood does not have a closed form. It involves an integral over the random effects (here, of dimension 4) which is numerically computed using an adaptive Gauss-Hermite quadrature with 3 integration points for this example.

As package \pkg{JM} includes an analytical computation of the gradient, we ran two estimations: one with the analytical gradient and one with the numerical approximation to compare the speed up and execution times.

### Latent class linear mixed model: package lcmm

The second example is a latent class linear mixed model, as implemented in the \code{hlme} function of the \pkg{lcmm} \proglang{R} package. The function uses a previous implementation of the Marquardt algorithm coded in \proglang{Fortran90} and in sequential mode. For the purpose of this example, we extracted the log-likelihood computation programmed in \proglang{Fortran90} to be used with \pkg{marqLevAlg} package. 

The latent class linear mixed model consists in two submodels estimated jointly: 

- a multinomial logistic regression for the latent class membership ($c_i$):

$$\mathbb{P}(c_i = g) = \frac{\exp(W_{i} \zeta_g)}{\sum_{l=1}^G \exp(W_{i} \zeta_l)}  ~~~~~~~~~~~~~ \text{with      }  g=1,...,G $$
where $\zeta_G=0$ for identifiability and $W_{i}$ contained an intercept and the 3 covariates. 

- a linear mixed model specific to each latent class $g$ for the repeated outcome $Y$ measured at times $t_{ij}$ ($j=1,...,n_i$): 

$$ Y_i(t_{ij} | c_i = g) =  X_i(t_{ij}) \beta_g + Z_i(t_{ij}) u_{ig} + \varepsilon_{ij}$$

where, in this example, $X_i(t)$ and $Z_i(t)$ contained an intercept, time $t$ and quadratic time. The vector $u_{ig}$ of correlated Gaussian random effects had a proportional variance across latent classes, and $\varepsilon_{ij}$ were independent Gaussian errors.

The log-likelihood of this model has a closed form but it involves the logarithm of a sum over latent classes which can become computationally demanding. We estimated the model on the total sample of $20,000$ subjects with 1, 2, 3 and 4 latent classes which corresponded to 10, 18, 26 and 34 parameters to estimate, respectively.


### Multivariate latent process mixed model: package CInLPN

The last example is provided by the \pkg{CInLPN} package, which relies on the \proglang{Rcpp} language. The function fits a multivariate linear mixed model combined with a system of difference equations in order to retrieve temporal influences between several repeated markers [@tadde_dynmod]. We used the data example provided in the package where three continuous markers \code{L_1}, \code{L_2}, \code{L_3} were repeatedly measured over time. The model related each marker $k$ ($k=1,2,3$) measured at observation times $t_{ijk}$ ($j=1,...,T$) to its underlying level $\Lambda_{ik}(t_{ijk}$ as follows: $$\text{L}_{ik}(t_{ijk}) = \eta_{0k}+ \eta_{1k} \Lambda_{ik}(t_{ijk}) +\epsilon_{ijk}$$
where $\epsilon_{ijk}$ are independent Gaussian errors and $(\eta_0,\eta_1)$ parameters to estimate. Simultaneously, the structural model defines the initial state at time 0 ($\Lambda_{ik}(0)$) and the change over time at subsequent times $t$ with $\delta$ is a discretization step:

$$
\begin{split}
 \Lambda_{ik}(0) &= \beta_{0k} + u_{ik}\\
 \frac{\Lambda_{ik}(t+\delta) - \Lambda_{ik}(t)}{\delta} &= \gamma_{0k} + v_{ik}  + \sum_{l=1}^K a_{kl} \Lambda_{il}(t)
\end{split} 
$$

where $u_{ik}$ and $v_{ik}$ are Gaussian random effects. 

Again, the log-likelihood of this model that depends on 27 parameters has a closed form but it may involve complex calculations. 

## Results

```{r, echo=FALSE, results='hide'}
load("res_JM_hlme_CInLPN.RData")

table1 <- cbind(c(40, 16, 40, round(temps_JM_nhs5000_analyDeriv[1, 1]), round(res_JM_nhs5000_analyDeriv[-1, 1], 2)),
  c(40, 16, 860, round(temps_JM_nhs5000_numeriDeriv[1, 1]), round(res_JM_nhs5000_numeriDeriv[-1, 1], 2)),
  c(10, 30, 65, round(temps_hlme_G1[1, 1]), round(res_hlme_G1[-1, 1], 2)),
  c(18, 30, 189, round(temps_hlme_G2[1, 1]), round(res_hlme_G2[-1, 1], 2)),
  c(26, 30, 377, round(temps_hlme_G3[1, 1]), round(res_hlme_G3[-1, 1], 2)),
  c(34, 30, 629, round(temps_hlme_G4[1, 1]), round(res_hlme_G4[-1, 1], 2)),
  c(27, 13, 405, round(temps_CInLPN[1, 1]), round(res_CInLPN[-1, 1], 2)))
rownames(table1) <- c("Number of parameters", "Number of iterations", "Number of elements in foreach loop", "Sequential time (seconds)",  "Speed up with 2 cores", "Speed up with 3 cores", "Speed up with 4 cores", "Speed up with 6 cores", "Speed up with 8 cores", "Speed up with 10 cores", "Speed up with 15 cores", "Speed up with 20 cores", "Speed up with 25 cores", "Speed up with 30 cores")
```

All the models have been estimated with 1, 2, 3, 4, 6, 8, 10, 15, 20, 25 and 30 cores. To fairly compare the execution times, we ensured that changing the number of cores did not affect the final estimation point or the number of iterations needed to converge. The mean of the speed up over the 100 replicates are reported in table  \ref{tab:perf} and plotted in Figure \ref{fig:speedup}. 

```{r perf, results='asis', echo=FALSE}
library("xtable")
addtorow <- list()
addtorow$pos <- list(-1, 0)
addtorow$command <- c("\\hline & \\multicolumn{2}{c}{JM} & \\multicolumn{4}{c}{hlme} & CInLPN \\\\", " & analytic & numeric & G=1 & G=2 & G=3 & G=4 & \\\\")
dig <- rbind(matrix(0, 4, 8), matrix(2, 10, 8))
print(xtable(table1, align=c("l", rep("r", 7)), digits = dig, label = "tab:perf",
             caption = "Estimation process characteristics for the 3 different programs (JM, hlme and CInLPN). Analytic and Numeric refer to the analytical and numerical computations of the gradient in JM; G refers to the number of latent classes."), 
      size = "\\small",
      comment = FALSE, add.to.row = addtorow, include.colnames = FALSE)
```



```{r speedup, echo=FALSE, results='hide', fig.width=11, fig.height=5.5, out.width='100%', fig.cap="Speed up performances for the 3 different programs (JM, hlme and CInLPN). Analytic and numeric refer to the analytical and numerical computations of the gradient in JM. The number of parameters was  40 for JM; 10, 18, 26, 34 for hlme with 1, 2, 3, 4 classes, respectively; 27 for CInLPN."}

pJM <- ggplot(data = cbind.data.frame("ncores" = rep(c(1, 2, 3, 4, 6, 8, 10, 15, 20, 25, 30), 2),
                               "speedup" = c(res_JM_nhs5000_numeriDeriv[, 1],
                                             res_JM_nhs5000_analyDeriv[, 1]),
                               "method" = factor(rep(c("Numeric gradient", "Analytical gradient"), each = 11), 
                                                 levels = c("Numeric gradient", "Analytical gradient"), ordered = TRUE),
                               "lowbound" = c(res_JM_nhs5000_numeriDeriv[, 1] - 1.96 * res_JM_nhs5000_numeriDeriv[, 2],
                                              res_JM_nhs5000_analyDeriv[, 1] - 1.96 * res_JM_nhs5000_analyDeriv[, 2]),
                               "upbound" = c(res_JM_nhs5000_numeriDeriv[, 1] + 1.96 * res_JM_nhs5000_numeriDeriv[, 2], 
                                             res_JM_nhs5000_analyDeriv[, 1] + 1.96 * res_JM_nhs5000_analyDeriv[, 2])
                               ),  
       aes(x = ncores)) +
  geom_ribbon(aes(ymin = lowbound, ymax = upbound, fill = method), alpha = 0.4) +
  geom_point(aes(y = speedup, colour = method)) +
  geom_line(aes(y = speedup, colour = method)) +
  theme_classic() +
  scale_x_continuous(minor_breaks = c(1, 2, 3, 4, 6, 8, 10, 15, 20, 25, 30), breaks = c(1, 5, 10, 15, 20, 25, 30)) +
  theme(panel.grid.major.y = element_line(), 
        panel.grid.minor.y = element_blank(),
        axis.title = element_text(size = 16),
        axis.text  = element_text(size = 14),
        title = element_text(size = 17, face = "bold"),
        legend.margin = margin(-30, 10, 10, 10),
        legend.position = "bottom", legend.direction = "vertical",
        legend.text = element_text(size = 14)) +
  xlab("Number of cores") +
  ylab("Speed-up") +
  scale_color_manual("", values = rev(viridis::inferno(5)[3:4])) +
  scale_fill_manual("", values = rev(viridis::inferno(5)[3:4])) +
  ggtitle("JM") +
  guides(fill="none") +
  NULL

phlme <- ggplot(data = cbind.data.frame("ncores" = rep(c(1, 2, 3, 4, 6, 8, 10, 15, 20, 25, 30), 4),
                               "speedup" = c(res_hlme_G1[, 1], res_hlme_G2[, 1],
                                             res_hlme_G3[, 1], res_hlme_G4[, 1]),
                               "nbclass" = factor(rep(c("1 class", "2 classes", "3 classes", "4 classes"), each = 11), 
                                                 levels = c("1 class", "2 classes", "3 classes", "4 classes"), ordered = TRUE),
                               "lowbound" = c(res_hlme_G1[, 1] - 1.96 * res_hlme_G1[, 2], res_hlme_G2[, 1] - 1.96 * res_hlme_G2[, 2],
                                              res_hlme_G3[, 1] - 1.96 * res_hlme_G3[, 2], res_hlme_G4[, 1] - 1.96 * res_hlme_G4[, 2]),
                               "upbound" = c(res_hlme_G1[, 1] + 1.96 * res_hlme_G1[, 2], res_hlme_G2[, 1] + 1.96 * res_hlme_G2[, 2],
                                              res_hlme_G3[, 1] + 1.96 * res_hlme_G3[, 2], res_hlme_G4[, 1] + 1.96 * res_hlme_G4[, 2])
                               ),  
       aes(x = ncores)) +
  geom_ribbon(aes(ymin = lowbound, ymax = upbound, fill = nbclass), alpha = 0.4) +
  geom_point(aes(y = speedup, colour = nbclass)) +
  geom_line(aes(y = speedup, colour = nbclass)) +
  theme_classic() +
  scale_x_continuous(minor_breaks = c(1, 2, 3, 4, 6, 8, 10, 15, 20, 25, 30), breaks = c(1, 5, 10, 15, 20, 25, 30)) +
  theme(panel.grid.major.y = element_line(), 
        panel.grid.minor.y = element_blank(),
        axis.title = element_text(size = 16),
        axis.text  = element_text(size = 14),
        title = element_text(size = 17, face = "bold"),
        legend.margin = margin(-30, 10, 10, 10),
        legend.position = "bottom", legend.direction = "vertical",
        legend.text = element_text(size = 14)) +
  xlab("Number of cores") +
  ylab("Speed-up") +
  scale_color_manual("", values = rev(viridis::viridis(5)[1:4])) +
  scale_fill_manual("", values = rev(viridis::viridis(5)[1:4])) +
  ggtitle("hlme") +
  guides(fill = "none", color = guide_legend(ncol = 2)) +
  NULL
  

pCInLPN <- ggplot(data = cbind.data.frame("ncores" = rep(c(1, 2, 3, 4, 6, 8, 10, 15, 20, 25, 30), 1),
                               "speedup" = res_CInLPN[, 1],
                               "lowbound" = res_CInLPN[, 1] - 1.96 * res_CInLPN[, 2],
                               "upbound" = res_CInLPN[, 1] + 1.96 * res_CInLPN[, 2]
                               ),  
       aes(x = ncores)) +
  geom_ribbon(aes(ymin = lowbound, ymax = upbound, fill = "95% Conf. Int."), alpha = 0.4) +
  geom_point(aes(y = speedup, color = "95% Conf. Int.")) +
  geom_line(aes(y = speedup, color = "95% Conf. Int.")) +
  theme_classic() +
  scale_x_continuous(minor_breaks = c(1, 2, 3, 4, 6, 8, 10, 15, 20, 25, 30), breaks = c(1, 5, 10, 15, 20, 25, 30)) +
  theme(panel.grid.major.y = element_line(), 
        panel.grid.minor.y = element_blank(),
        axis.title = element_text(size = 16),
        axis.text  = element_text(size = 14),
        title = element_text(size = 17, face = "bold"),
        legend.margin = margin(-10, 10, 10, 10),
        legend.position = "bottom",
        legend.text = element_text(size = 14)) +
  scale_color_manual("", values = "black") +
  scale_fill_manual("", values = "black") +
  xlab("Number of cores") +
  ylab("Speed-up") +
  ggtitle("CInLPN") +
  NULL

(pJM + ylim(0, 18)) + (phlme + ylim(0, 18)) + (pCInLPN + ylim(0, 18))
```


The joint shared random effect model (\code{JM}) converged in 16 iterations after 4279 seconds in sequential mode when using the analytical gradient. Running the algorithm in parallel on 2 cores made the execution 1.85 times shorter. Computational time was gradually reduced with a number of cores between 2 and 10 to reach a maximal speed up slightly above 4. With 15, 20, 25 or 30 cores, the performances were no more improved, the speed up showing even a slight reduction, probably due to the overhead. In contrast, when the program involved numerical computations of the gradient, the parallelization reduced the computation time by a factor of almost 8 at maximum. The better speed-up performances with a numerical gradient calculation were expected since the parallel loops iterate over more elements. 

The second example, the latent class mixed model estimation (\code{hlme}), showed an improvement of the performances as the complexity of the models increased. The simple linear mixed model (one class model), like the joint models with analytical gradient, reached a maximum speed-up of 4 with 10 cores. The two class mixed model with 18 parameters, showed a maximum speed up of 7.71 with 20 cores. Finally the 3 and 4 class mixed models reached speed-ups of 13.33 and 17.89 with 30 cores and might still be improved with larger resources.

The running time of the third program (CInLPN) was also progressively reduced with the increasing number of cores reaching the maximal speed-up of 8.36 for 20 cores. 


In these 7 examples, the speed up systematically reached almost 2 with 2 cores, and it remained interesting with 3 or 4 cores although some variations in the speed-up performances began to be observed according to the complexity of the objective function computations. This hilights the benefit of the parallel implementation of MLA even on personal computers. As the number of cores continued to increase, the speed-up performances varied a lot. Among our examples, the most promising situation was the one of the latent class mixed model (with program in \proglang{Fortran90}) where the speed-up was up to 15 for 20 cores with the 4 class model. 



# Comparison with other optimization algorithms

The \pkg{JM} package (@rizopoulos_jm_2010) includes several optimization algorithms, namely the BFGS of \code{optim} function, and an expectation-maximization technique internally implemented. It thus offers a nice framework to compare the reliability of MLA to find the maximum likelihood of a joint model with the reliability of other optimization algorithms. We used in this comparison the \code{prothro} dataset described in the \pkg{JM} package and elsewhere [@skrondal_generalized_2004,@andersen_statistical_1993]. It consists of a randomized trial in which 488 subjects were split into two treatment arms (prednisone *versus* placebo). Repeated measures of prothrombin ratio were collected over time as well as time to death. 
The longitudinal part of the joint model included a linear trajectory with time in the study, an indicator of first measurement and their interaction with treatment group. Were also included correlated individual random effects on the intercept and the slope with time. The survival part was a proportional hazard model adjusted for treatment group as well as the dynamics of the longitudinal outcome either through the current value of the marker or its slope or both. The baseline risk function was approximated by B-splines with one internal knot. The total number of parameters to estimate was 17 or 18 (10 for the longitudinal submodel, and 7 for the survival submodel considering only the curent value of the marker or its slope or 8 for the survival model when both the current level and the slope were considered). The marker initially ranged from 6 to 176 (mean=79.0, sd=27.3). To investigate the consistency of the results to different dimensions of the marker, we also considered cases where the marker was rescaled by a factor 0.1 or 10. In these cases, the log-likelihood was rescaled a posteriori to the original dimension of the marker to make the comparisons possible. The starting point was systematically set at the default initial value of the \code{jointModel} function, which is the estimation point obtained from the separated linear mixed model and proportional hazard model. 
Codes and dataset used in this section are available at \url{https://github.com/VivianePhilipps/marqLevAlgPaper}.

MLA runned on 3 cores and converged when the three criteria defined in section \ref{sec:criteria} were satisfied with tolerance 0.001, 0.001 and 0.01 for the parameters, the likelihood and the RDM, respectively. BFGS converged when the convergence criterion on the log-likelihood was satisfied with the square root of the tolerance of the machine ($\approx 10^{-8}$). The EM algorithm converged when stability on the parameters or on the log-likelihood was satisfied with tolerance 0.0001 and around $10^{-8}$ (i.e., the square root of the tolerance of the machine), respectively.


```{r, echo=FALSE, results='hide', message = FALSE}
library("JM")

prothro$t0 <- as.numeric(prothro$time == 0)
lmeFit <- lme(pro ~ treat * (time + t0), random = ~ time | id, data = prothro)
survFit <- coxph(Surv(Time, death) ~ treat, data = prothros, x = TRUE)

dFpro <- list(fixed = ~ 1 + treat, indFixed = c(3, 5), random = ~ 1, indRandom = 2)

prothro$pro01 <- prothro$pro / 10
lmeFit01 <- lme(pro01 ~ treat * (time + t0), random = ~ time | id, data = prothro)

prothro$pro10 <- prothro$pro * 10
lmeFit10 <- lme(pro10 ~ treat * (time + t0), random = ~ time | id, data = prothro)

load("jm_pro.RData") 
```

```{r prothro, results='asis', echo=FALSE, message = FALSE}

a <- expand.grid(c("", "01", "10"), c("val", "slo", "both"), "GH15", c("BFGS", "marq", "EM"), stringsAsFactors=FALSE)
mod <- apply(a, 1, function(x) paste("jm_pro", paste(x, collapse = "_"), sep = ""))

abis <- expand.grid(c("", "01", "10"), c("val"), "GH15", c("BFGS", "marq", "EM"), stringsAsFactors = FALSE)
modbis <- apply(abis, 1, function(x) paste("jm_pro", paste(x, collapse = "_"), sep = ""))
ater <- expand.grid(c("", "01", "10"), c("slo"), "GH15", c("BFGS", "marq", "EM"), stringsAsFactors = FALSE)
modter <- apply(ater, 1, function(x) paste("jm_pro", paste(x, collapse = "_"), sep = ""))

res <- matrix(NA, 27, 7)
k <- 0
for(m in mod)
{
  k <- k + 1
  jm <- get(m)
  
  sc <- 1
  if(length(grep("pro10", m))) sc <- 10
  if(length(grep("pro01", m))) sc <- 0.1
  
  res[k, 1] <- jm$convergence
  res[k, 2] <- jm$iters
  res[k, 3] <- jm$logLik
  res[k, 4] <- jm$logLik + nrow(prothro) * log(sc)
  if(m %in% modbis) res[k, 7] <- NA
  else res[k, 7] <- as.numeric(jm$coefficients$Dalpha)
  if(m %in% modter) res[k, 6] <- NA
  else  res[k, 6] <- as.numeric(jm$coefficients$alpha)
  if(!is.null(jm$qNtime)) res[k, 5] <- jm$qNtime[3]
  if(!is.null(jm$EMtime)) res[k, 5] <- jm$EMtime
}

colnames(res) <- c("convergence", "iterations", "LogLik", "scaledLogLik", "time", "alpha", "Dalpha")

library("xtable")

res2 <- data.frame(res, algorithm = 0, scaling = 0, association = 0, value = 0, slope = 0)

res2$algorithm[c(1:9)] <- "BFGS"
res2$algorithm[c(10:18)] <- "MLA"
res2$algorithm[c(19:27)] <- "EM"
res2$scaling[c(1, 4, 7, 10, 13, 16, 19, 22, 25)] <- "1"
res2$scaling[c(2, 5, 8, 11, 14, 17, 20, 23, 26)] <- "0.1"
res2$scaling[c(3, 6, 9, 12, 15, 18, 21, 24, 27)] <- "10"
res2$association[c(1:3, 10:12, 19:21)] <- "value"
res2$association[c(4:6, 13:15, 22:24)] <- "slope"
res2$association[c(7:9, 16:18, 25:27)] <- "both"

res2$alpha[c(2, 5, 8, 20, 23, 26, 11, 14, 17)] <- res2$alpha[c(2, 5, 8, 20, 23, 26, 11, 14, 17)] / 10
res2$alpha[c(3, 6, 9, 21, 24, 27, 12, 15, 18)] <- res2$alpha[c(3, 6, 9, 21, 24, 27, 12, 15, 18)] * 10
res2$Dalpha[c(2, 5, 8, 20, 23, 26, 11, 14, 17)] <- res2$Dalpha[c(2, 5, 8, 20, 23, 26, 11, 14, 17)] / 10
res2$Dalpha[c(3, 6, 9, 21, 24, 27, 12, 15, 18)] <- res2$Dalpha[c(3, 6, 9, 21, 24, 27, 12, 15, 18)] * 10


res2$value[c(1, 2, 3, 19, 20, 21, 10, 11, 12)] <- 100 * (res2$alpha[c(1, 2, 3, 19, 20, 21, 10, 11, 12)] - res2$alpha[10]) / res2$alpha[10]
res2$slope[c(1, 2, 3, 19, 20, 21, 10, 11, 12)] <- 100 * (res2$Dalpha[c(1, 2, 3, 19, 20, 21, 10, 11, 12)] - res2$Dalpha[10]) / res2$Dalpha[10]

res2$value[c(4, 5, 6, 22, 23, 24, 13, 14, 15)] <- 100 * (res2$alpha[c(4, 5, 6, 22, 23, 24, 13, 14, 15)] - res2$alpha[13]) / res2$alpha[13]
res2$slope[c(4, 5, 6, 22, 23, 24, 13, 14, 15)] <- 100 * (res2$Dalpha[c(4, 5, 6, 22, 23, 24, 13, 14, 15)] - res2$Dalpha[13]) / res2$Dalpha[13]

res2$value[c(7, 8, 9, 25, 26, 27, 16, 17, 18)] <- 100 * (res2$alpha[c(7, 8, 9, 25, 26, 27, 16, 17, 18)] - res2$alpha[16]) / res2$alpha[16]
res2$slope[c(7, 8, 9, 25, 26, 27, 16, 17, 18)] <- 100 * (res2$Dalpha[c(7, 8, 9, 25, 26, 27, 16, 17, 18)] - res2$Dalpha[16]) / res2$Dalpha[16]

resbis <- res2[c(1, 2, 3, 19, 20, 21, 10, 11, 12, 4, 5, 6, 22, 23, 24, 13, 14, 15, 7, 8, 9, 25, 26, 27, 16, 17, 18),
              c("association", "algorithm", "scaling", "LogLik", "scaledLogLik", "value", "slope", "iterations", "time")]
	      
addtorow <- list()
addtorow$pos <- list(-1, 0)
addtorow$command <- c("\\hline  Nature of & Algorithm & Scaling & Rescaled log-& Variation of& Variation of& Number of & Time in\\\\",
  " dependency & & factor & likelihood & value (\\%) & slope (\\%) & iterations & seconds \\\\")

print(xtable(resbis[, -c(4)], align = rep("r", 9), digits = c(1, 2, 2, 1, 2, 2, 2, 0, 2), label = "tab:prothro",
             caption = "Comparison of the convergence obtained by MLA, BFGS and EM algorithms for the estimation of a joint model for prothrobin repeated marker (scaled by 1, 0.1 or 10) and time to death when considering a dependency on the current level of prothrobin ('value') or the current slope ('slope') or both ('both'). All the models converged correctly according to the algorithm outputs. We report the final log-likelihood rescaled to scaling factor 1 (for comparison), the  percentage of variation of the association parameters ('value' and 'slope' columns) compared to the one obtained with the overall maximum likelihood with scaling 1, the number of iterations and the running time in seconds. "),
      comment = FALSE, include.rownames = FALSE, include.colnames = FALSE, add.to.row = addtorow, hline.after = c(0, nrow(resbis)), size = "footnotesize")
```




Table \ref{tab:prothro} compares the convergence obtained by using the three optimization methods, when considering a pseudo-adaptive Gauss-Hermite quadrature with 15 points. All the algorithms converged correctly according to the programs. Although the model for a given association structure is exactly the same, some differences were observed in the final maximum log-likelihood (computed in the original scale of prothrombin ratio). The final log-likelihood obtained by MLA was always the same whatever the outcome's scaling, showing its consistency. It was also higher than the one obtained using the two other algorithms, showing that BFGS and, to a lesser extent, EM did not systematically converge toward the effective maximum. The difference could go up to 20 points of log-likelihood for BFGS in the example with the current slope of the marker as the association structure. The convergence also differed according to outcome's scaling with BFGS and slightly with EM, even though in general the EM algorithm seemed relatively stable in this example. The less stringent convergence of BFGS and, to a lesser extent, of EM had also consequences on the parameters estimates as roughly illustrated in Table \ref{tab:prothro} with the percentage of variation in the association parameters of prothrombin dynamics estimated in the survival model (either the current value or the current slope) in comparison with the estimate obtained using MLA which gives the overall maximum likelihood. 
The better performances of MLA was not at the expense of the number of iterations since MLA converged in at most 22 iterations, whereas several hundreds of iterations could be required for EM or BFGS. Note however that one iteration of MLA is much more computationally demanding. 

Finally, for BFGS, the problem of convergence is even more apparent when the outcome is scaled by a factor 10. Indeed, the optimal log-likelihood of the model assuming a bivariate association structure (on the current level and the current slope) is worse than the optimal log-likelihood of its nested model which assumes an association structure only on the current level (i.e., constraining the parameter for the current slope to 0). 

# Concluding remarks

We proposed in this paper a general-purpose optimization algorithm based on a robust Marquardt-Levenberg algorithm. The program, written in \proglang{R} and \proglang{Fortran90}, is available in \pkg{marqLevAlg} \proglang{R} package. It provides a very nice alternative to other optimization packages available in \proglang{R} software such as \pkg{optim}, \pkg{roptim} [@pan_2020] or \pkg{optimx}  [@nash_2011]. In particular, as shown in our example with the estimation of joint models, it is more reliable than classical alternatives (EM and BFGS). This is due to the very good convergence properties of the Marquardt-Levenberg algorithm associated with very stringent convergence criteria based on the first and second derivatives of the objective function which avoids spurious convergence at saddle points [@commenges_rvs_2006]. 

The Marquardt-Levenberg algorithm is known for its very computationally intensive iterations due to the computation of the first and second derivatives. However, first, compared to other algorithms, it converges in a very small number of iterations (usually less than 30 iterations). Second, we implemented parallel computations of the derivatives which can largely speed up the program and make it competitive with alternatives in terms of running time. 

We chose in our implementation to rely on RDM criterion which is a very stringent convergence criteria. As it is based on the inverse of the Hessian matrix, it may cause non-convergence issues when some parameters are at the border of the parameter space (for instance 0 for a parameter contrained to be positive). In that case, we recommend to fix the parameter at the border of the parameter space and run again the optimization on the rest of the parameters. In cases where the stabilities of the log-likelihood and of the parameters are considered sufficient to ensure satisfactory convergence, the program outputs might be interpreted despite a lack of convergence according to the RDM.

As any other optimization algorithm based on the steepest descent, MLA does not ensure the convergence of multimodal objective functions toward the global maximum. 

Despite the complexity of many statistical models, general-purpose optimizers in \proglang{R} were all constrained to sequential mode to our knowledge, despite increasingly powerful computers and servers. There is an exception with the Broyden-Fletcher-Goldfarb-Shanno algorithm with box constraints (L-BFGS-B) which was very recently made available in a parallel mode using the independent computations of objective function in each iteration [@gerber_2019]. With its parallel implementation of derivative calculations combined with very good convergence properties of MLA, \pkg{marqLevAlg} package provides a promising solution for the estimation of complex statistical models in \proglang{R}. We have chosen for the moment to parallelize the derivatives which is very useful for optimization problems involving many parameters. However we could also easily parallelize the computation of the objective function when the latter is decomposed into independent sub-computations as is the log-likelihood computed independently on the statistical units. This alternative is currently under development. 


# Funding

This work was funded by French National Research Agency [grant number ANR-18-CE36-0004-01 for project DyMES] and [grant number ANR-2010-PRPS-006 for project MOBYDIQ]. 

# Acknowlegdments

The computing facilities MCIA (Mésocentre de Calcul Intensif Aquitain) at the Université de Bordeaux and the Université de Pau et des Pays de l’Adour provided advice on parallel computing technologies, as well as computer time.


